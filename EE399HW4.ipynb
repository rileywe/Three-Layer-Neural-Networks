{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riley Estes\n",
    "EE399 HW4\n",
    "5/4/2023\n",
    "\n",
    "Github link:\n",
    "https://github.com/rileywe/Three-Layer-Neural-Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x:\\anaconda\\lib\\site-packages\\sklearn\\datasets\\_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Creates array data\n",
    "X_nums=np.arange(0,31)\n",
    "Y_nums=np.array([30, 35, 33, 32, 34, 37, 39, 38, 36, 36, 37, 39, 42, 45, 45, 41,\n",
    "40, 39, 42, 44, 47, 49, 50, 49, 46, 48, 50, 53, 55, 54, 53])\n",
    "\n",
    "# Imports MNIST data\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X_mnist, Y_mnist = mnist.data, mnist.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes three layer feed forward neural network with a size 9 hidden layer\n",
    "three_layer_NN = MLPRegressor(hidden_layer_sizes=(9,), activation='relu', solver='adam', max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits the given model to the training data\n",
    "# Calculates the training and test mean squared errors as well as the least squared errors\n",
    "# for each point in both datasets\n",
    "def modelErrors (model, X_train, X_test, Y_train, Y_test):\n",
    "    model.fit(X_train.reshape(-1, 1), Y_train)\n",
    "\n",
    "    # LSE and MSE for training data\n",
    "    Y_pred_train = model.predict(X_train.reshape(-1, 1))\n",
    "\n",
    "    # Calculates LSE for each training data point\n",
    "    LSE_train = (Y_train - Y_pred_train)**2\n",
    "\n",
    "    # Prints LSE for each training data point\n",
    "    for i in range(len(X_train)):\n",
    "        print(\"Training data point {}: LSE = {:.2f}\".format(i, LSE_train[i]))\n",
    "\n",
    "    mse_train = mean_squared_error(Y_train, Y_pred_train)\n",
    "    print(\"MSE on training data: {:.2f}\".format(mse_train))\n",
    "\n",
    "    print()\n",
    "\n",
    "    # LSE and MSE for testing data\n",
    "    Y_pred_test = model.predict(X_test.reshape(-1, 1))\n",
    "\n",
    "    # Calculates LSE for each testing data point\n",
    "    LSE_test = (Y_test - Y_pred_test)**2\n",
    "\n",
    "    # Prints LSE for each testing data point\n",
    "    for i in range(len(X_test)):\n",
    "        print(\"Testing data point {}: LSE = {:.2f}\".format(i, LSE_test[i]))\n",
    "\n",
    "    mse_test = mean_squared_error(Y_test, Y_pred_test)\n",
    "    print(\"MSE on test data: {:.2f}\".format(mse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data point 0: LSE = 4.16\n",
      "Training data point 1: LSE = 5.05\n",
      "Training data point 2: LSE = 0.22\n",
      "Training data point 3: LSE = 4.40\n",
      "Training data point 4: LSE = 0.52\n",
      "Training data point 5: LSE = 2.75\n",
      "Training data point 6: LSE = 9.23\n",
      "Training data point 7: LSE = 2.01\n",
      "Training data point 8: LSE = 1.45\n",
      "Training data point 9: LSE = 3.33\n",
      "Training data point 10: LSE = 2.09\n",
      "Training data point 11: LSE = 0.00\n",
      "Training data point 12: LSE = 5.34\n",
      "Training data point 13: LSE = 21.99\n",
      "Training data point 14: LSE = 16.55\n",
      "Training data point 15: LSE = 0.31\n",
      "Training data point 16: LSE = 4.73\n",
      "Training data point 17: LSE = 14.41\n",
      "Training data point 18: LSE = 2.01\n",
      "Training data point 19: LSE = 0.00\n",
      "MSE on training data: 5.03\n",
      "\n",
      "Testing data point 0: LSE = 2.64\n",
      "Testing data point 1: LSE = 5.11\n",
      "Testing data point 2: LSE = 3.55\n",
      "Testing data point 3: LSE = 0.24\n",
      "Testing data point 4: LSE = 23.61\n",
      "Testing data point 5: LSE = 17.90\n",
      "Testing data point 6: LSE = 12.98\n",
      "Testing data point 7: LSE = 3.90\n",
      "Testing data point 8: LSE = 1.81\n",
      "Testing data point 9: LSE = 13.82\n",
      "Testing data point 10: LSE = 37.07\n",
      "MSE on test data: 11.15\n"
     ]
    }
   ],
   "source": [
    "# Fits the model using the first 20 points for training\n",
    "X_train, X_test = X_nums[:20], X_nums[20:]\n",
    "Y_train, Y_test = Y_nums[:20], Y_nums[20:]\n",
    "\n",
    "three_layer_NN.fit(X_train.reshape(-1, 1), Y_train)\n",
    "\n",
    "modelErrors(three_layer_NN, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data point 0: LSE = 0.00\n",
      "Training data point 1: LSE = 14.17\n",
      "Training data point 2: LSE = 0.23\n",
      "Training data point 3: LSE = 3.24\n",
      "Training data point 4: LSE = 1.17\n",
      "Training data point 5: LSE = 1.37\n",
      "Training data point 6: LSE = 5.99\n",
      "Training data point 7: LSE = 0.53\n",
      "Training data point 8: LSE = 3.98\n",
      "Training data point 9: LSE = 7.38\n",
      "Training data point 10: LSE = 2.66\n",
      "Training data point 11: LSE = 3.64\n",
      "Training data point 12: LSE = 0.04\n",
      "Training data point 13: LSE = 12.49\n",
      "Training data point 14: LSE = 5.09\n",
      "Training data point 15: LSE = 0.95\n",
      "Training data point 16: LSE = 1.70\n",
      "Training data point 17: LSE = 6.66\n",
      "Training data point 18: LSE = 0.74\n",
      "Training data point 19: LSE = 0.74\n",
      "MSE on training data: 3.64\n",
      "\n",
      "Testing data point 0: LSE = 5.94\n",
      "Testing data point 1: LSE = 1.34\n",
      "Testing data point 2: LSE = 1.25\n",
      "Testing data point 3: LSE = 11.55\n",
      "Testing data point 4: LSE = 7.17\n",
      "Testing data point 5: LSE = 4.18\n",
      "Testing data point 6: LSE = 14.17\n",
      "Testing data point 7: LSE = 30.09\n",
      "Testing data point 8: LSE = 10.28\n",
      "Testing data point 9: LSE = 3.72\n",
      "MSE on test data: 8.97\n"
     ]
    }
   ],
   "source": [
    "# Fits the model using the first and last 10 points as training\n",
    "X_train = np.concatenate((X_nums[:10], X_nums[-10:]))\n",
    "Y_train = np.concatenate((Y_nums[:10], Y_nums[-10:]))\n",
    "X_test = X_nums[10:20]\n",
    "Y_test = Y_nums[10:20]\n",
    "\n",
    "three_layer_NN.fit(X_train.reshape(-1, 1), Y_train)\n",
    "\n",
    "modelErrors(three_layer_NN, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data point 0: LSE = 0.05\n",
      "Training data point 1: LSE = 6.09\n",
      "Training data point 2: LSE = 0.16\n",
      "Training data point 3: LSE = 4.57\n",
      "Training data point 4: LSE = 0.76\n",
      "Training data point 5: LSE = 1.95\n",
      "Training data point 6: LSE = 7.09\n",
      "Training data point 7: LSE = 0.86\n",
      "Training data point 8: LSE = 3.25\n",
      "Training data point 9: LSE = 6.43\n",
      "Training data point 10: LSE = 2.79\n",
      "Training data point 11: LSE = 3.75\n",
      "Training data point 12: LSE = 0.04\n",
      "Training data point 13: LSE = 12.46\n",
      "Training data point 14: LSE = 5.12\n",
      "Training data point 15: LSE = 0.99\n",
      "Training data point 16: LSE = 1.62\n",
      "Training data point 17: LSE = 6.45\n",
      "Training data point 18: LSE = 0.65\n",
      "Training data point 19: LSE = 0.86\n",
      "MSE on training data: 3.30\n",
      "\n",
      "Testing data point 0: LSE = 5.15\n",
      "Testing data point 1: LSE = 1.00\n",
      "Testing data point 2: LSE = 1.60\n",
      "Testing data point 3: LSE = 12.48\n",
      "Testing data point 4: LSE = 7.84\n",
      "Testing data point 5: LSE = 3.74\n",
      "Testing data point 6: LSE = 13.44\n",
      "Testing data point 7: LSE = 29.15\n",
      "Testing data point 8: LSE = 9.81\n",
      "Testing data point 9: LSE = 3.48\n",
      "MSE on test data: 8.77\n"
     ]
    }
   ],
   "source": [
    "# Fits the model to the test data in the middle\n",
    "three_layer_NN.fit(X_test.reshape(-1, 1), Y_test)\n",
    "\n",
    "modelErrors(three_layer_NN, X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the models fit in homework one to the neural network:\n",
    "These models both perform worse on the data set than the simpler linear and sinusoidal regression models. \n",
    "For the first dataset where the first 20 points are used for training, the test MSE is 11.15 for the neural network, and 3.53 for the linear model. For the second dataset where the first and last 10 points are used for training, the test MSE is 8.97 for the neural network, and 2.95 for the linear model. When fitted to the test data instead, the training MSE for the neural network is 3.30, which is comparable to the linear fit, except that the test MSE is still very bad. The linear regression model performs much better in both cases than the neural network at a fraction of the computational cost, making it the obvious choice for simple curve-fitting tasks such as these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the first 20 PCA modes of the MNIST data\n",
    "pca = PCA(n_components=20)\n",
    "X_pca = pca.fit_transform(X_mnist)\n",
    "\n",
    "# Splits the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_pca, Y_mnist, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feed forward NN test accuracy: 95.69%\n"
     ]
    }
   ],
   "source": [
    "# Trains a 3 layer feedforward neural network on the PCA data\n",
    "three_layer_mnist = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000, random_state=0)\n",
    "three_layer_mnist.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluates the performance of the model on the test data\n",
    "accuracy = three_layer_mnist.score(X_test, Y_test)*100\n",
    "print(\"Feed forward NN test accuracy: {:.2f}%\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "438/438 [==============================] - 14s 27ms/step - loss: 1.9329 - accuracy: 0.3121\n",
      "Epoch 2/10\n",
      "438/438 [==============================] - 12s 28ms/step - loss: 1.2290 - accuracy: 0.5823\n",
      "Epoch 3/10\n",
      "438/438 [==============================] - 11s 24ms/step - loss: 0.8600 - accuracy: 0.7159\n",
      "Epoch 4/10\n",
      "438/438 [==============================] - 10s 22ms/step - loss: 0.6710 - accuracy: 0.7827\n",
      "Epoch 5/10\n",
      "438/438 [==============================] - 10s 23ms/step - loss: 0.5811 - accuracy: 0.8129\n",
      "Epoch 6/10\n",
      "438/438 [==============================] - 10s 23ms/step - loss: 0.5234 - accuracy: 0.8342\n",
      "Epoch 7/10\n",
      "438/438 [==============================] - 10s 23ms/step - loss: 0.4726 - accuracy: 0.8515\n",
      "Epoch 8/10\n",
      "438/438 [==============================] - 10s 23ms/step - loss: 0.4387 - accuracy: 0.8618\n",
      "Epoch 9/10\n",
      "438/438 [==============================] - 10s 22ms/step - loss: 0.4180 - accuracy: 0.8691\n",
      "Epoch 10/10\n",
      "438/438 [==============================] - 10s 24ms/step - loss: 0.3771 - accuracy: 0.8819\n",
      "LSTM test accuracy: 86.56%\n"
     ]
    }
   ],
   "source": [
    "# Trains and evaluates a 3 layer LSTM with a size 100 hidden layer, 10 epochs, and batch size of 128 on the MNIST data\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(128, input_shape=(X_train.shape[1], 1)))\n",
    "model_lstm.add(Dense(10, activation='softmax'))\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "X_train_lstm = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "model_lstm.fit(X_train_lstm, np.eye(10)[Y_train.astype(int)], epochs=10, batch_size=128)\n",
    "_, acc_lstm = model_lstm.evaluate(X_test.reshape((X_test.shape[0], X_test.shape[1], 1)), np.eye(10)[Y_test.astype(int)], verbose=0)\n",
    "print(\"LSTM test accuracy: {:.2f}%\".format(acc_lstm*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test accuracy: 97.30%\n"
     ]
    }
   ],
   "source": [
    "# Trains and evaluates an SVM on the MNIST data\n",
    "model_svm = SVC()\n",
    "model_svm.fit(X_train, Y_train)\n",
    "acc_svm = model_svm.score(X_test, Y_test)*100\n",
    "print(\"SVM test accuracy: {:.2f}%\".format(acc_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision tree test accuracy: 84.41%\n"
     ]
    }
   ],
   "source": [
    "# Trains and evaluates a decision tree on the MNIST data\n",
    "model_dt = DecisionTreeClassifier()\n",
    "model_dt.fit(X_train, Y_train)\n",
    "acc_dt = model_dt.score(X_test, Y_test)*100\n",
    "print(\"Decision tree test accuracy: {:.2f}%\".format(acc_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
